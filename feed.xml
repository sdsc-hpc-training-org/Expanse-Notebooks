<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-12-18T17:09:23-08:00</updated><id>https://hpc-training.sdsc.edu//Expanse-Notebooks/feed.xml</id><title type="html">HPC Training Notebooks</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Session 4.2 R on HPC Demo</title><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/r-on-hpc/" rel="alternate" type="text/html" title="Session 4.2 R on HPC Demo" /><published>2025-06-25T00:00:00-07:00</published><updated>2025-06-25T00:00:00-07:00</updated><id>https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/r-on-hpc</id><content type="html" xml:base="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/r-on-hpc/"><![CDATA[<h1 id="session-42-r-on-hpc-demo">Session 4.2 R on HPC Demo</h1>

<p><strong>Date:</strong> Thursday, June 26, 2025</p>

<p><strong>Summary</strong>: A presentation and demo of parallelizing R; also an example case study of several ML tools and R for big data.</p>

<p><strong>Presented by:</strong> <a href="https://profiles.ucsd.edu/paul.rodriguez">Paul Rodriguez</a> (p4rodriguez at ucsd.edu)</p>

<h3 id="reading-and-presentations">Reading and Presentations:</h3>
<ul>
  <li><strong>Lecture material:</strong></li>
  <li><strong>Presentation slides:</strong>
    <h3 id="tasks-none-at-this-time">TASKS: None at this time.</h3>
  </li>
</ul>

<p><a href="#top">Back to Top</a></p>]]></content><author><name></name></author><category term="expanse" /><category term="CIML-2025" /><category term="R" /><summary type="html"><![CDATA[Session 4.2 A presentation and demo of parallelizing R; also an example case study of several ML tools and R...]]></summary></entry><entry><title type="html">Session 4.4 LLM Overview</title><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/llm-overview/" rel="alternate" type="text/html" title="Session 4.4 LLM Overview" /><published>2025-06-25T00:00:00-07:00</published><updated>2025-06-25T00:00:00-07:00</updated><id>https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/llm-overview</id><content type="html" xml:base="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/llm-overview/"><![CDATA[<h1 id="session-44-llm-overview">Session 4.4 LLM Overview</h1>

<p><strong>Date:</strong> Thursday, June 26, 2025</p>

<p><strong>Summary</strong>: In this session we will present an introduction to Large Language Models and the possible use cases. Examples of how to use LLMs will be covered.  This session is designed for people with a basic understanding of machine learning but no prior knowledge of LLMs is required.
<strong>Agenda</strong></p>
<ul>
  <li>LLM/GenAI Intro - Bob Sinkovits (30 min)</li>
  <li>Prompt Engineering w/ Hands-On - Mai Nguyen (60 min)</li>
  <li>RAG - Mai Nguyen (30 min)</li>
  <li>LLM-related tools - Paul Rodriguez (30 min)</li>
  <li>Other Topics (ethical issues, etc.) - Paul Rodriguez &amp; Mai Nguyen (30 min)</li>
</ul>

<p><strong>Presented by:</strong></p>
<ul>
  <li><a href="https://www.sdsc.edu/research/researcher_spotlight/nguyen_mai.html">Mai Nguyen</a> (mhnguyen at ucsd.edu)</li>
  <li><a href="https://profiles.ucsd.edu/paul.rodriguez">Paul Rodriguez</a> (p4rodriguez at ucsd.edu)</li>
  <li><a href="https://www.sdsc.edu/research/researcher_spotlight/sinkovits_robert.html">Robert Sinkovits</a> (rssinkovits at ucsd.edu)</li>
</ul>

<h3 id="reading-and-presentations">Reading and Presentations:</h3>
<ul>
  <li><strong>Lecture material:</strong></li>
  <li><strong>Presentation slides:</strong>
    <h3 id="tasks-none-at-this-time">TASKS: None at this time.</h3>
  </li>
</ul>

<p><a href="#top">Back to Top</a></p>




<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/4-4/openai-api.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>






<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/4-4/RAG_Solution.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>






<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/4-4/RAG_Starter.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>]]></content><author><name></name></author><category term="expanse" /><category term="CIML-2025" /><category term="Machine-Learning" /><category term="AI-ML" /><summary type="html"><![CDATA[Session 4.4 In this session we will present an introduction to Large Language Models and the possible use cases]]></summary></entry><entry><title type="html">Session 3.6 Deep Learning – Special Connections</title><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/special-connections/" rel="alternate" type="text/html" title="Session 3.6 Deep Learning – Special Connections" /><published>2025-06-25T00:00:00-07:00</published><updated>2025-06-25T00:00:00-07:00</updated><id>https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/special-connections</id><content type="html" xml:base="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/special-connections/"><![CDATA[<h1 id="session-36-deep-learning--special-connections">Session 3.6 Deep Learning – Special Connections</h1>

<p><strong>Date:</strong> Wednesday, June 25, 2025</p>

<p><strong>Summary</strong>: The architecture of many networks use paths and connections in flexible ways; we will review gate, skip, and residual connections and get some intuition what they are good for.</p>

<p><strong>Presented by:</strong> <a href="https://profiles.ucsd.edu/paul.rodriguez">Paul Rodriguez</a> (p4rodriguez at ucsd.edu)</p>

<h3 id="reading-and-presentations">Reading and Presentations:</h3>
<ul>
  <li><strong>Presentation slides:</strong></li>
</ul>

<h3 id="tasks-none-at-this-time">TASKS: None at this time.</h3>

<p><a href="#top">Back to Top</a></p>




<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/3-6/CIML25_MNIST_AE_wskip_v7.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>






<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/3-6/CIML25_ToyAttn_v6.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>]]></content><author><name></name></author><category term="expanse" /><category term="CIML-2025" /><category term="Machine-Learning" /><category term="AI-ML" /><summary type="html"><![CDATA[Session 3.6 The architecture of many networks use paths and connections in flexible ways; we will review gate...]]></summary></entry><entry><title type="html">Session 3.3 Practical Guidelines for Training Deep Learning on HPC</title><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/practical-multinode/" rel="alternate" type="text/html" title="Session 3.3 Practical Guidelines for Training Deep Learning on HPC" /><published>2025-06-25T00:00:00-07:00</published><updated>2025-06-25T00:00:00-07:00</updated><id>https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/practical-multinode</id><content type="html" xml:base="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/practical-multinode/"><![CDATA[<h1 id="session-33-practical-guidelines-for-training-deep-learning-on-hpc">Session 3.3 Practical Guidelines for Training Deep Learning on HPC</h1>

<p><strong>Date:</strong> Wednesday, June 25, 2025</p>

<p><strong>Summary</strong>: Guildelines on running deep networks on Expanse, such as using tensorboard, notebooks, and batch jobs; also some discussion of multinode execution.</p>

<p><strong>Presented by:</strong> <a href="https://profiles.ucsd.edu/paul.rodriguez">Paul Rodriguez</a> (p4rodriguez at ucsd.edu)</p>

<h3 id="reading-and-presentations">Reading and Presentations:</h3>
<ul>
  <li><strong>Presentation slides:</strong></li>
  <li>
    <h3 id="tasks-none-at-this-time">TASKS: None at this time.</h3>
  </li>
</ul>

<p><a href="#top">Back to Top</a></p>]]></content><author><name></name></author><category term="expanse" /><category term="CIML-2025" /><category term="Machine-Learning" /><category term="AI-ML" /><summary type="html"><![CDATA[Session 3.3 Practical Multinode]]></summary></entry><entry><title type="html">Session 3.1 Machine Learning (ML) Experiment Tracking</title><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/machine-learning/" rel="alternate" type="text/html" title="Session 3.1 Machine Learning (ML) Experiment Tracking" /><published>2025-06-25T00:00:00-07:00</published><updated>2025-06-25T00:00:00-07:00</updated><id>https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/machine-learning</id><content type="html" xml:base="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/machine-learning/"><![CDATA[<h1 id="session-31-machine-learning-ml-experiment-tracking">Session 3.1 Machine Learning (ML) Experiment Tracking</h1>

<p><strong>Date:</strong> Wednesday, June 25, 2025</p>

<p><strong>Summary</strong>: Introduction to tools for ML experiment tracking.</p>

<p><strong>Presented by:</strong> <a href="https://www.sdsc.edu/research/researcher_spotlight/nguyen_mai.html">Mai Nguyen</a> (mhnguyen at ucsd.edu)</p>




<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/3-1/logger_extraction_ptl.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>



<p><a href="#top">Back to Top</a></p>]]></content><author><name></name></author><category term="expanse" /><category term="CIML-2025" /><category term="Machine-Learning" /><category term="AI-ML" /><summary type="html"><![CDATA[Session 3.1 Introduction to tools for ML experiment tracking.]]></summary></entry><entry><title type="html">Session 3.5 Deep Learning Transfer Learning</title><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/deep-transfer-learning/" rel="alternate" type="text/html" title="Session 3.5 Deep Learning Transfer Learning" /><published>2025-06-25T00:00:00-07:00</published><updated>2025-06-25T00:00:00-07:00</updated><id>https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/deep-transfer-learning</id><content type="html" xml:base="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/deep-transfer-learning/"><![CDATA[<h1 id="session-35-deep-learning-transfer-learning">Session 3.5 Deep Learning Transfer Learning</h1>

<p><strong>Date:</strong> Wednesday, June 25, 2025</p>

<p><strong>Summary</strong>: Overview of deep learning concepts, including layers, architectures, applications, and libraries.</p>

<p><strong>Presented by:</strong> <a href="https://www.sdsc.edu/research/researcher_spotlight/nguyen_mai.html">Mai Nguyen</a> (mhnguyen at ucsd.edu)</p>

<h3 id="reading-and-presentations">Reading and Presentations:</h3>
<ul>
  <li><strong>Lecture material:</strong></li>
  <li><strong>Presentation slides:</strong>
    <h3 id="tasks-none-at-this-time">TASKS: None at this time.</h3>
  </li>
</ul>

<h1 id="notebooks-are-attached-below">Notebooks are attached below:</h1>




<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/3-5/feature_extraction_ptl_Solution.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>



<p><a href="#top">Back to Top</a></p>




<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/3-5/finetune_ptl_Solution.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>






<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/3-5/feature_extraction_ptl.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>






<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/3-5/finetune_ptl.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>]]></content><author><name></name></author><category term="expanse" /><category term="CIML-2025" /><category term="Machine-Learning" /><category term="AI-ML" /><summary type="html"><![CDATA[Session 3.5 Overview of deep learning concepts, including layers, architectures, applications, and libraries]]></summary></entry><entry><title type="html">Session 3.2 Introduction to Neural Networks and Convolution Neural Networks</title><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/intro-to-neural-networks/" rel="alternate" type="text/html" title="Session 3.2 Introduction to Neural Networks and Convolution Neural Networks" /><published>2025-06-25T00:00:00-07:00</published><updated>2025-06-25T00:00:00-07:00</updated><id>https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/intro-to-neural-networks</id><content type="html" xml:base="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/intro-to-neural-networks/"><![CDATA[<h1 id="32-introduction-to-neural-networks-and-convolution-neural-networks">3.2 Introduction to Neural Networks and Convolution Neural Networks</h1>

<p><strong>Date:</strong> Wednesday, June 25, 2025</p>

<p><strong>Summary</strong>: An overview of the main concepts of neural networks and feature discovery; the basic convolution neural network for digit recognition using tensorflow.</p>

<p><strong>Presented by:</strong> <a href="https://profiles.ucsd.edu/paul.rodriguez">Paul Rodriguez</a> (p4rodriguez at ucsd.edu)</p>

<h3 id="reading-and-presentations">Reading and Presentations:</h3>
<ul>
  <li><strong>Presentation slides:</strong></li>
</ul>

<h3 id="tasks-none-at-this-time">TASKS: None at this time.</h3>




<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/3-2/CIML25_MNIST_Intro_v6.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>



<p><a href="#top">Back to Top</a></p>]]></content><author><name></name></author><category term="expanse" /><category term="CIML-2025" /><category term="Machine-Learning" /><category term="AI-ML" /><summary type="html"><![CDATA[Session 3.2 An overview of the main concepts of neural networks and feature discovery; the basic convolution neural...]]></summary></entry><entry><title type="html">Session 4.3 Spark</title><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/spark/" rel="alternate" type="text/html" title="Session 4.3 Spark" /><published>2025-06-25T00:00:00-07:00</published><updated>2025-06-25T00:00:00-07:00</updated><id>https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/spark</id><content type="html" xml:base="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/spark/"><![CDATA[<h1 id="session-43-spark">Session 4.3 Spark</h1>

<p><strong>Date:</strong> Thursday, June 26, 2025</p>

<p><strong>Summary</strong>: Introduction to performing machine learning at scale, with hands-on exercises using Spark.</p>

<p><strong>Presented by:</strong> <a href="https://www.sdsc.edu/research/researcher_spotlight/nguyen_mai.html">Mai Nguyen</a> (mhnguyen at ucsd.edu)</p>

<h3 id="reading-and-presentations">Reading and Presentations:</h3>
<ul>
  <li><strong>Lecture material:</strong></li>
  <li><strong>Presentation slides:</strong>
    <h3 id="tasks-none-at-this-time">TASKS: None at this time.</h3>
  </li>
</ul>

<p><a href="#top">Back to Top</a></p>




<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/4-3/pyspark-clustering-soln.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>






<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/4-3/pyspark-clustering-wOutput.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>






<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/4-3/pyspark-clustering.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>






<div
  class="jupyter-notebook"
  style="position: relative; width: 100%; margin: 0 auto;">
  <div class="jupyter-notebook-iframe-container">
    <iframe
      src="/Expanse-Notebooks/assets/jupyter/CIML_2025/4-3/pyspark-demo.ipynb.html"
      style="position: absolute; top: 0; left: 0; border-style: none;"
      width="100%"
      height="100%"
      onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe>
  </div>
</div>]]></content><author><name></name></author><category term="expanse" /><category term="CIML-2025" /><category term="Machine-Learning" /><category term="AI-ML" /><summary type="html"><![CDATA[Session 4.3 An introduction to performing machine learning at scale, with hands-on exercises using Spark.]]></summary></entry><entry><title type="html">Session 3.4 Deep Learning Layers and Architectures</title><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/deep-learning/" rel="alternate" type="text/html" title="Session 3.4 Deep Learning Layers and Architectures" /><published>2025-06-25T00:00:00-07:00</published><updated>2025-06-25T00:00:00-07:00</updated><id>https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/deep-learning</id><content type="html" xml:base="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/deep-learning/"><![CDATA[<h1 id="session-34-deep-learning-layers-and-architectures">Session 3.4 Deep Learning Layers and Architectures</h1>

<p><strong>Date:</strong> Wednesday, June 25, 2025</p>

<p><strong>Summary</strong>: Overview of deep learning concepts, including layers, architectures, applications, and libraries.</p>

<p><strong>Presented by:</strong> <a href="https://www.sdsc.edu/research/researcher_spotlight/nguyen_mai.html">Mai Nguyen</a> (mhnguyen at ucsd.edu)</p>

<h3 id="reading-and-presentations">Reading and Presentations:</h3>
<ul>
  <li><strong>Lecture material:</strong></li>
  <li><strong>Presentation slides:</strong>
    <h3 id="tasks-none-at-this-time">TASKS: None at this time.</h3>
  </li>
</ul>

<p><a href="#top">Back to Top</a></p>]]></content><author><name></name></author><category term="expanse" /><category term="CIML-2025" /><category term="Machine-Learning" /><category term="AI-ML" /><summary type="html"><![CDATA[Session 3.4 Overview of deep learning concepts, including layers, architectures, applications, and libraries.]]></summary></entry><entry><title type="html">GPU Computing - Hardware architecture and software infrastructure</title><link href="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/GPU-computing/" rel="alternate" type="text/html" title="GPU Computing - Hardware architecture and software infrastructure" /><published>2025-06-24T00:00:00-07:00</published><updated>2025-06-24T00:00:00-07:00</updated><id>https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/GPU-computing</id><content type="html" xml:base="https://hpc-training.sdsc.edu//Expanse-Notebooks/blog/2025/GPU-computing/"><![CDATA[<h1 id="session-25-gpu-computing---hardware-architecture-and-software-infrastructure">Session 2.5 GPU Computing - Hardware architecture and software infrastructure</h1>

<p><strong>Date:</strong> Tuesday, June 24, 2025</p>

<p><strong>Summary</strong>: Brief overview of the massively parallel GPU architecture that enables large-scale deep learning applications, access and use of GPUs on SDSC Expanse for ML applications</p>

<p><strong>Presented by:</strong> <a href="https://www.sdsc.edu/research/researcher_spotlight/goetz_andreas.html">Andreas Goetz</a> (awgoetz @ucsd.edu)</p>

<h3 id="reading-and-presentations">Reading and Presentations:</h3>
<ul>
  <li><strong>Lecture material:</strong>
    <ul>
      <li>Presentation Slides: <a href="./CIML-SI25-GPU-Computing.pdf">GPU Computing - Hardware Architecture and Software Infrastructure</a></li>
    </ul>
  </li>
  <li><strong>Source Code/Examples:</strong> <a href="https://github.com/NVIDIA/cuda-samples">Nvidia CUDA Samples</a></li>
</ul>

<h3 id="tasks">TASKS:</h3>

<p>We will log into an Expanse GPU node, compile and test some examples from the Nvidia CUDA samples.</p>

<h3 id="log-into-expanse-get-onto-a-shared-gpu-node-and-load-required-modules">Log into Expanse, get onto a shared GPU node, and load required modules</h3>

<p>First, log onto Expanse using your <code class="language-plaintext highlighter-rouge">train</code> training account. You can do this either via the Expanse user portal or simply using ssh:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh trainXXX@login.expanse.sdsc.edu
</code></pre></div></div>

<p>Next we will use the alias for the <code class="language-plaintext highlighter-rouge">srun</code> command that is defined in your <code class="language-plaintext highlighter-rouge">.bashrc</code> file to access a single GPU on a shared GPU node:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>srun-gpu-shared
</code></pre></div></div>

<p>Once we are on a GPU node, we load the default modules for the GPU nodes to gain access to the GPU software stack. We will also load the CUDA toolkit:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module reset
module load cuda12.2/toolkit
module list
</code></pre></div></div>
<p>You should see following output</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Currently Loaded Modules:
  1) shared            3) slurm/expanse/23.02.7   5) DefaultModules
  2) gpu/0.17.3b (g)   4) sdsc/1.0                6) cuda12.2/toolkit/12.2.2

  Where:
   g:  built natively for Intel Skylake

</code></pre></div></div>

<p>We can use the <code class="language-plaintext highlighter-rouge">nvidia-smi</code> command to check for available GPUs and which processes are running on the GPU.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi
</code></pre></div></div>
<p>You should have a single V100 GPU available and there should be no processes running:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mon Jun 27 08:39:33 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0    41W / 300W |      0MiB / 32768MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre></div></div>

<h3 id="copy-the-nvidia-cuda-samples-into-the-local-scratch-directory">Copy the Nvidia CUDA samples into the local scratch directory</h3>

<p>We will first copy the Nvidia CUDA samples from the shared CIML Summer Institute Data directory. If we work in the home directory, then this operation will take a few minutes, in particular if there are many participants copying the data at the same time, as there are many files to copy. You can try to do this in the home directory to see how much time it takes.</p>

<p>Instead of using the home directory we will use the node-local scratch directory. Data in this directory will be purged upon job completion but this is OK as we do not need to retain the toolkit samples. Working in the the scratch directory will be very fast. This is also important when you operate with datasets and/or have heavy I/O during your ML applications.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd /scratch/$USER/job_$SLURM_JOB_ID
tar xvf $CIML25_DATA_DIR/cuda-samples-v12.2.tar.gz
</code></pre></div></div>
<p>You can look into the data directory to see if there other samples that are of interest to you. The CUDA samples have been obtained from <a href="https://github.com/nvidia/cuda-samples">Nvidia’s Github repository</a>.</p>

<p>For convenience, we will store the location of the CUDA samples in an environment variable:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export CUDA_SAMPLES=/scratch/$USER/job_$SLURM_JOB_ID/cuda-samples-12.2
</code></pre></div></div>

<p>We are now ready to look at the CUDA samples.
It can be instructive to look at the source code if you want to learn about CUDA.</p>

<h3 id="compile-and-run-the-devicequery-cuda-sample">Compile and run the <code class="language-plaintext highlighter-rouge">deviceQuery</code> CUDA sample</h3>

<p>The first sample we will look at is <code class="language-plaintext highlighter-rouge">device_query</code>. This is a utility that demonstrates how to query Nvidia GPU properties. It often comes in handy to check information on the GPU that you have available.</p>

<p>First, we check that we have an appropriate NVIDIA CUDA compiler available. The CUDA samples require at least version 11.7. Because we loaded the CUDA Toolkit module above, we should have the <code class="language-plaintext highlighter-rouge">nvcc</code> compiler available:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvcc --version
</code></pre></div></div>
<p>should give the following output</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Tue_Aug_15_22:02:13_PDT_2023
Cuda compilation tools, release 12.2, V12.2.140
Build cuda_12.2.r12.2/compiler.33191640_0
</code></pre></div></div>

<p>We have version 12.2 installed so we are good to go. 
We can now move into the <code class="language-plaintext highlighter-rouge">device_query</code> source directory and compile the code with the <code class="language-plaintext highlighter-rouge">make</code> command.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd $CUDA_SAMPLES/Samples/1_Utilities/deviceQuery
make
</code></pre></div></div>

<p>You now should have an executable <code class="language-plaintext highlighter-rouge">deviceQuery</code> in the directory. If you execute it:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./deviceQuery
</code></pre></div></div>
<p>you should see an output with details about the GPU that is available. In our case on Expanse it is a Tesla V100-SXM2-32GB GPU:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: "Tesla V100-SXM2-32GB"
  CUDA Driver Version / Runtime Version          11.6 / 11.2
  CUDA Capability Major/Minor version number:    7.0
  Total amount of global memory:                 32511 MBytes (34089926656 bytes)
  (080) Multiprocessors, (064) CUDA Cores/MP:    5120 CUDA Cores
  GPU Max Clock rate:                            1530 MHz (1.53 GHz)
  Memory Clock rate:                             877 Mhz
  Memory Bus Width:                              4096-bit
  L2 Cache Size:                                 6291456 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total shared memory per multiprocessor:        98304 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 5 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Managed Memory:                Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.6, CUDA Runtime Version = 11.2, NumDevs = 1
Result = PASS
</code></pre></div></div>

<h3 id="compile-and-run-the-matrix-multiplication">Compile and run the matrix multiplication</h3>

<p>It is instructive to look at two different matrix multiplication examples and compare the performance.</p>

<p>First we will look at a hand-written matrix multiplication. This implementation features several performance optimizations such as tiling to minimize data transfer from GPU RAM to the GPU processors and thus increasing arithmetic intensity and overall floating point performance.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd $CUDA_SAMPLES/Samples/0_Introduction/matrixMul
make
</code></pre></div></div>
<p>We now have the executable <code class="language-plaintext highlighter-rouge">matrixMul</code> available. If we execute it,</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./matrixMul
</code></pre></div></div>
<p>a matrix multiplication will be performed and the performance reported</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Matrix Multiply Using CUDA] - Starting...
GPU Device 0: "Volta" with compute capability 7.0

MatrixA(320,320), MatrixB(640,320)
Computing result using CUDA Kernel...
done
Performance= 2796.59 GFlop/s, Time= 0.047 msec, Size= 131072000 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
</code></pre></div></div>

<h3 id="compile-and-run-matrix-multiplication-with-cublas-library">Compile and run matrix multiplication with CUBLAS library</h3>

<p>Finally, let us look at a matrix multiplication that uses Nvidia’s CUBLAS library, which is a highly optimized version of the Basic Linear Algebra System for Nvidia GPUs.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd $CUDA_SAMPLES/Samples/4_CUDA_Libraries/matrixMulCUBLAS
make
</code></pre></div></div>
<p>If we run the executable</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./matrixMulCUBLAS
</code></pre></div></div>
<p>we should get following output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Matrix Multiply CUBLAS] - Starting...
GPU Device 0: "Volta" with compute capability 7.0

GPU Device 0: "Tesla V100-SXM2-32GB" with compute capability 7.0

MatrixA(640,480), MatrixB(480,320), MatrixC(640,320)
Computing result using CUBLAS...done.
Performance= 7032.97 GFlop/s, Time= 0.028 msec, Size= 196608000 Ops
Computing result using host CPU...done.
Comparing CUBLAS Matrix Multiply with CPU results: PASS

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
</code></pre></div></div>

<p>How does the performance compare to the hand written (but optimized) matrix multiplication?</p>

<p><a href="#top">Back to Top</a></p>]]></content><author><name></name></author><category term="expanse" /><category term="CIML-2025" /><category term="GPU" /><category term="Slurm" /><category term="CUDA" /><category term="NVIDIA" /><summary type="html"><![CDATA[Topic 2.5- Brief overview of the massively parallel GPU architecture that enables large-scale deep learning...]]></summary></entry></feed>